## 数据仓库是什么

目的：面向分析和决策

***意义：数据仓库的意义就是对企业所有的数据进行汇总，为各个部门提供统一的、规范数据出口***

- 稳定性：来源于历史数据，不存在更新删除
- 集合性：数据来自多个数据源，需要整合与清洗
- 大数据量：数据仓库存储的数据量大，需要支持大量数据分析
- 面向主题：数据围绕业务主题，把与主题相关的数据整合起来形成一个整体。

## 数仓架构

**大数据组件**

![img](https://raw.githubusercontent.com/winway/picRepo/master/pics/20220513104722.png)

## 数仓建模

| 层级                           | 建设思路                                                     | 作用                                                         | 设计原理                                                     | 数据处理                                                 | 表类型                                               |
| ------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------------------------------------------- | ---------------------------------------------------- |
| ODS 原始数据层                 | 存放原始数据，后端数据快照。                                 | 这一层的主要职责是将基础数据同步、存储。                     |                                                              |                                                          | 1.从业务库直接同步的数据<br />2.直接接入原始日志数据 |
| DWD 数据明细层<br />(原子指标) | 从ODS获取的数据按照`主题建立`数据模型，存放明细事实表。<br />对原始数据进行清理和一致性处理 | 业务过程驱动，按照最细粒度指标存储<br />划分数据主题域，面向主题<br />定期同步 | 选择业务过程：履约主题域的到课参课<br />定义粒度：课程(小节)+用户<br />确认维度：学员信息；课程信息；时间信息；商品信息<br />确认事实：学习持续时间；进度完成情况；出勤情况<br />关联维度 | 空值处理；验证业务数据有效性；规范数据格式；统一数据标准 | 事务性事实表，周期性快照事实宽表，累计快照事实宽表   |
| DWS 数据汇总层<br />(派生指标) | 分析的主题对象为驱动，构建公共粒度的汇总指标事实表。<br />构建命名规范和口径统一的统计指标 | 满足90%的共性需求，解决口径不一致问题<br />宽表化处理        | 确认聚集维度：计算粒度商品粒度<br />确认统计周期：天、周<br />确认聚集事实：派生指标，订单的总额 | 指标汇总；指标合并                                       | 公共汇总宽表                                         |
| DIM 维度层                     |                                                              | 建立一致性维度<br />降低数据计算口径和算法不统一风险。       | 选择维度：保证维度唯一性 <br />确定主维表：一般是ODS表，直接与业务系统同步 确定相关维度：确定哪些表和主维表存在关联关系，并选择其中的某些表用于生成维度属性。 <br />确定维度属性：从主维表或者相关维度表选择维度属性 |                                                          | 1.高基数维度表<br />2.低基数维度表                   |
| ADS 应用层                     |                                                              | 个性化指标业务                                               |                                                              |                                                          |                                                      |

**分层的好处：**

- 清晰的数据结构：每一个层级都有作用域和职责，通过明确的层次结构，可以更方便的定位以及追踪数据的流向
- 减少重复开发：规范数据分层，根据不同的主题或者数据域存储，数据解耦，减少重复计算
- 统一的数据口径：数据分层提供统一的数据口径，统一对外输出的口径
- 性能优化：不同层次的数据可以针对特定的查询和分析任务优化；把数据查询和处理操作分散到不同的层级，减少单一负载

## 主题域、数据域划分

**主题域** 一个业务主题或者业务过程的数据集合。

**数据域** 按照数据性质分类的数据集合，按照业务的板块的功能模块划分

例如教育业务是一个主题域，数据域有 交易，流量，渠道，内容，用户，履约

区别：数据域对数据的分类，主题域是对业务的分类

## DWS层怎么设计

确认聚集维度：业务关心的数据维度+数据的粒度

确认统计周期

确认聚集事实：

- 基于数据维度/表粒度进行轻度汇总，计算派生指标

**设计原则**

聚集是不跨越事实的：聚集是针对原始星形模型进行的汇总

聚集的维度和度量必须与原始模型保持一致

子类目对应的一级类目发生变更时，先前存在的、已经被汇总到聚集表中的数据需要被重新调整。

## 数仓VS数据库

|          | 数据库                                                         | 数据仓库                                                                                                                                |
| -------- | -------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| 数据结构 | 规范化的数据结构，二维表的关系型数据库                         | 非规范化或部分规范化的数据结构，如星形模式（star schema）和雪花模式（snowflake schema），这些结构优化了数据读取速度，便于进行多维分析。 |
| 目的     | 处理日常业务,On line Transaction Processing.支持高并发快速响应 | 存储历史数据，支持复杂查询，报表分析，On line Analytical Processing                                                                     |
| 数据更新 | 增删改查，经常发生                                             | 批量进行，日，周，月更新。没有修改和删除                                                                                                |
| 数据源   | 单一数据源，例如上课时间记录表                                 | 多数据源                                                                                                                                |

## 数仓模型

### OLTP OLAP

- 操作型处理 OLTP 主要执行基本日常的事物处理
- 分析型处理 OLAP 针对某些主题的历史数据分析

### ER模型

实体-关系模型，Bill Inmon建模方法

优点：保证数据的一致性和冗余少，应用于OLTP系统（操作型处理）中

### 维度模型

通常以一个事实表为中心进行表的组织，面向业务

 1、星型模型
 以事实表为中心，所有维度表直接连在事实表上面。数据组织直观、执行效率高
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/d1f8c4c435554f01a4f09787e91a4938.png)
 2、雪花模型
 维度表可以有其他的维度表，维护成本高
![在这里插入图片描述](https://img-blog.csdnimg.cn/8cb988075cc64e68be527d35d6c82033.png)
 3、星座模型
基于多张事实表，建设一致性维度表，多张事实表共享维度信息。
![在这里插入图片描述](https://img-blog.csdnimg.cn/72d32693cd6a40bc99d56b23d42654f3.png)

### Data Value 模型

DataVault由Hub（关键核心业务实体）、Link（关系）、Satellite（实体属性） 三部分组成

它是在ER关系模型上的衍生，同时设计的出发点也是为了实现数据的整合，并非为数据决策分析直接使用。

### Anchor模型

高度可扩展的模型，所有的扩展只是添加而不是修改，因此它将模型规范到6NF，基本变成了K-V结构模型。

## 范式建模(Inmon建模)

3NF设计规范，符合实体-关系模型

结合业务系统，构建实体数据-实体关系的数据表，一份数据只保存在一个地方，没有数据冗余

- 特点：主要解决数据冗余的问题

### 三大范式

| 范式 | 概念                                                                         |
| ---- | ---------------------------------------------------------------------------- |
| 1NF  | 原子性，数据库中每一列都不可分                                               |
| 2NF  | 所有建依赖于主键（可能是混合主键）                                           |
| 3NF  | 每个属性与主键有直接关系，消除传递依赖。（eg：学生表与专业表完全无传值依赖） |

## 维度建模(Kimball建模)

概念：Kimball建模，把数据抽取为维度+事实表。事实表存储数值化的度量，多个维度表围绕事实表，事实表通过外键连接维度表。

数据冗余不能保证数据口径一致

- 事实表
  - 存储业务相关的量化数据。组成：**度量**（量化的数组，例如成本）+**外键**(连接到`维度表`的键)
  - 事实表的主键一般是组合健，表达多对多的关系。

- 维度表
  - 维度表包含了事实表中指定属性的相关详细信息
  - 组成：**主键**(唯一)+ 其他描述性属性


### **维度建模设计**

**1、选择业务过程**

业务过程相关的维度描述 和 每个业务过程关联的描述性环境，获取关键性能度量

**2、声明粒度**

粒度：精确定义事实表的每一行所表示的业务含义

**3、确认维度**

完成了粒度声明，就意味着确定了主键，对应的维度组合以及相关的维度字段也可以确定了

**4、确认事实**

选择与业务过程有关的所有事实，且事实的粒度要与所声明的事实表的粒度一致；

### 事实表种类

1. 单事务事实表

   一个阶段的业务过程设计一个事实表。教育履约小节直播明细表

2. 多事务事实表

   不同阶段的业务过程汇总在一张事实表。教育全链路大款表

3. 周期快照事实表

   在确定的时间间隔内，对指标项统计，获得周期性度量值。期数到课完课表（对每周的指标进行一次汇总（完课人数、到课人数、退课人数、加微量、触达人数、退费数量）、助教服务表 当月的各种指标）。

4. 累计快照事实表

   不确定的时间间隔内，对指标项进行统计。例如淘宝的下单-支付-确认收货-评价时间

### 维度表设计方法

1. 选择维度：保证维度唯一性
2. 确定主维表：一般是ODS表，直接与业务系统同步
3. 确定相关维度：确定哪些表和主维表存在关联关系，并选择其中的某些表用于生成维度属性。
4. 确定维度属性：从主维表或者相关维度表选择维度属性

## 退化维度

没有对应的维度表，存储在事实表中。如说订单编号，你可以获得这个订单里面包含哪些商品，对应的商家是谁，下单的用户是谁；分析商品，商家的进行分组统计订单数。

## 维度拆分和整合

**整合** 整合相关属性，命名规范和字段编码含义一致

**拆分** 拆分成雪花模型

## 拉链表（缓慢变化维）

缓慢变化维：维度的属性可能随时间流逝变化。使用代理键做维度表的主键。

1. 直接覆盖原值

   ![img](https://img-blog.csdnimg.cn/img_convert/e1bd30254a92928b27c9a4915429a903.png)
2. 拉链表

   增加**开链时间**，**关链时间**和行标识

   ![](https://img-blog.csdnimg.cn/img_convert/c3d0a73aa36ef44db1604087f63ad11e.png)
3. 增加属性列

   存储旧标识和新标识

   ![](https://img-blog.csdnimg.cn/img_convert/776905e38e4e75d457a37ea5e33afd99.png)

## 增量表、全量表、快照表和拉链表

1、全量表：对所有的数据每个分区存一份

2、增量表：只存储当前分区的增量数据

3、快照表：存储的是历史到当前时间的数据。

4、拉链表：对于表中部分字段会被update、查看某一时间段的历史快照、变化比例和频率不是很大的数。
例如，物权表，用户的物权变化是缓慢变化维，采用加行的方式。

## OneData建模过程

![](https://img-blog.csdnimg.cn/19cd9e2827a547f0970c043176710045.png)

## 数仓质量  (模型建设\数据质量)

**数仓质量评估方法**

1. 模型建设
   - 完善度
     - DWD层完善度：`DWD的引用率`和`跨层引用率`（ODS直接被DWS/ADS/DM层引用） 
     - DWS/ADS/DM层完善度：汇总数据能直接满足多少查询需求
   - 复用度
     - 模型的引用系统：比如⼀张DWD层表被5张DWS层表引⽤，这张DWD层表的引⽤系数就是5。
     - 复用性好的模型是交织的发散型结构
   - 规范度
     - 规范的表命名：主题域、分层、全量/增量等信息
     - 表属性命名在不同模型是否一致
2. 存储和运行性能
   - 表的生命周期
   - 运行时长管理(数据倾斜情况)
   - DQC运行失败率
3. 数据质量

`一致性`： 数据格式规范+字典范围

`完整性`：数据量校验+主键唯一(重复校验)+空数据校验

`准确性`：业务规则校验（pv>=uv>=ip）+ 异常特征识别（掉0风险）

`及时性`：基于任务调度日志的时间效率监控

`规范性`：表的命名，字段命名，数据的存储格式规范。

`唯一性`：表中不存在重复数据。例如dwd层存在重复数据，dws层的汇总指标计算可能有误

![img](https://img-blog.csdnimg.cn/b5fb91f864664c738f2b6ff21c20940b.png)



**数仓质量保障方法**

1. 数据一致性校验

   源数据vs ODS 行数，字段值一致

   ODS vs DWD 主键一致，字段值一致

   DWD vs DWS 聚合前后汇总校验

   DWS vs 报表 业务展示与数仓指标一致

   维度对齐 是否使用标准维度表

2. 建模设计阶段

   粒度定义：事实表和维表的粒度文档化

   分层规范

## 指标规范

* **原子指标** ：也称为基本指标，最基本的度量，通常直接来源于原始数据或经过基本处理的数据
  - 原子指标内置的计算逻辑有：**求和、均值、计数、去重计数、最大值、最小值等**
* **派生指标** ：派生指标是基于一个或多个原子指标经过数学运算或逻辑运算得到的指标。(通常在dws)
  * **原子指标 + 时间周期+ 业务限定（修饰词） = 派生指标**
  * 平均订单价值（每日总销售额 / 订单数量）、广告投资回报率（ROI）（每日总销售额 / 广告费用）。

* **复合指标** ：比率类、均分类、占比类

- 指标常见问题：
  1. 相同指标名称，口径不一致
  2. 相同口径，指标名称不一致
  3. 不同限定词，描述相同事实过程的两个指标，相同事实部分口径不一致
  4. 指标口径描述不清楚
  5. 指标口径描述错误
  6. 指标命名难于理解
  7. 指标数据来源和计算逻辑不清晰
- 核心思想：
  - ⾯向主题域管理，指标中的主题域与数仓中的概念是⼀致的
  - 拆分原⼦指标和派⽣指标，统计周期、统计粒度、业务限定、原⼦指标，组成派⽣指标，所以原⼦指标可以定义为不能够按照上述规则进⼀步拆分的指标。
  - 指标命名规范，做到易懂/统一，重点要能体现主题/业务过程/维度限定/业务限定/时间周期

## 指标、维度、度量、粒度

**指标：** 量化的度量，由**度量**计算得出，例如转化率。

**维度：** 表中数据对象的特征和属性，来源于维表。用与对**度量**进行分组聚合来计算**指标**

**度量：** 事实表中的数值属性字段

**粒度：**每一行所表示的业务含义，由表中一些维度决定

## 元数据

描述数据的数据

1. 描述性元数据（Descriptive Metadata） 描述性元数据主要是用来描述数据的内容、形式和结构等方面的信息。比如，它可以包括关于数据的标题、作者、主题、摘要、关键字、日期等信息，以帮助用户快速了解和找到所需的数据。
2. 技术元数据（Technical Metadata） 技术元数据主要是用来描述数据的技术特性和属性的信息。比如，它可以包括数据的文件格式、编码方式、大小、分辨率、数据源、数据格式等信息，以帮助系统和应用程序理解和处理数据。
3. 行政元数据（Administrative Metadata） 行政元数据主要是用来管理和维护数据的信息。比如，它可以包括数据的创建时间、访问权限、版本号、维护人员、存储位置等信息，以帮助管理和维护数据的使用和安全。

## 开窗函数

- 聚合函数：sum() ,avg(),max(),min()

  ```sql
  SELECT SUM(Sales) OVER (ORDER BY Date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS Cumulative_Sales FROM sales_data;
  ```
- 排名函数：row_number(),rank(),dense_rank()

  ````sql
  100,200,200,300
  rn   1,2,3,4
  rank 1,2,2,4
  dr   1,2,2,3         SELECT  DENSE_RANK() OVER (ORDER BY Score DESC) AS Rank  FROM student_scores;(从大到小)
  ````
- 分析函数：lead(),lag(),FIRST_VALUE(),LAST_VALUE()

  ```sql
  lead(参数，n，Default)            查看下一行数据      SELECT lead(name) OVER (ORDER BY id) FROM employees;
  lag(参数，n，Default)             查看上一行数据      SELECT lag(name) OVER (ORDER BY id) FROM employees;
  FIRST_VALUE(参数)     查看第一行数据      SELECT FIRST_VALUE(name) OVER (ORDER BY hire_date) FROM employees;
  LAST_VALUE(参数)      查看最后一行数据    SELECT LAST_VALUE(name) OVER (ORDER BY hire_date ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) FROM employees;（必须要指定rows）
  ```

**语法：**

```sql
SELECT ROW_NUMBER() OVER(
    PARTITION BY COLUMN4 
    ORDER BY COLUMN5
    [ROWS|RANGE BETWEEN A AND B]
) AS colunm_name
FROM TABLE
WHERE CONDITION
```

`PARTITION BY` 用于定义窗口内数据分组的依据

`ORDER BY` 用于数据排序（**ASC `默认`：**由小到大  **DESC**:由大到小 ）

`ROWS|RANGE` 定义了窗口的前后范围

rows&range：

```sql
1、完整分区窗口
ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING: 包括整个分区的所有行

2、到当前分区末尾的窗口
ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING: 从当前行到分区的最后一行。

3、动态调整窗口
ROWS BETWEEN 1 PRECEDING AND CURRENT ROW: 这个窗口包括当前行和它之前的一行，用于计算这两行的数据。
ROWS BETWEEN 3 PRECEDING AND 1 FOLLOWING: 包括当前行、前面三行和后面一行，合计五行数据。

5、固定距离的窗口（基于逻辑值如日期）
RANGE BETWEEN INTERVAL 1 DAY PRECEDING AND CURRENT ROW: 对于日期字段，这个窗口包括从当前日期向前数一天内的所有行。
RANGE BETWEEN INTERVAL 1 HOUR PRECEDING AND INTERVAL 1 HOUR FOLLOWING: 包括从当前时间前后各一小时内的所有行。
```

## **SQL执行顺序：**

- FROM
- JOIN
- WHERE
- GROUP BY
- 聚合函数
- HAVING
- SELECT
- DISTINCT
- ORDER BY
- LIMIT/OFFSET

## 炸裂函数

| 函数                      |                                                              | 使用方法                                                     |
| ------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| explode( map 或 array)    |                                                              | explode(arrays) AS array<br />explode(map) AS (key, value)<br />explode(get_json_object(json, '$.tags')) AS tag |
| posexplode( map 或 array) | position+explode                                             | LATERAL VIEW posexplode(array) t AS pos, element             |
| Lateral View              | lateral view udtf(expression) <br />虚拟表别名 as col1 [,col2,col3……] | lateral view explode(split(列名,'分隔符')) t as value        |

```sql
select tf1.*, tf2.*
from table
lateral view explode(map('A',10,'B',20,'C',30)) tf1 as key,value
lateral view explode(map('A',10,'B',20,'C',30)) tf2 as key2,value2
where tf1.key = tf2.key2;
```

## 集合函数

| 函数                   |                                  |                                                  |
| ---------------------- | -------------------------------- | ------------------------------------------------ |
| Split(string str, ''） | 分割字符串，返回array            | split('qazwsxedc','s');         >["qazw","xedc"] |
| collect_list (列)      | 多行合并，返回array              | collect_list(video_name)[0]                      |
| collect_set（列）      | 多行合并，去重，返回array        |                                                  |
| concat(str1,str2,...)  | 多个字符串连接成一个，返回string |                                                  |
| concat(分隔符，列，列) | 分割符号连接                     |                                                  |
| coalesce（T1,T2）      | 返回第一个非空值                 |                                                  |

## JSON函数

**get_json_object**(SPARK)

提取单个HIVE列中json字符串

```
SELECT get_json_object('{"a":{"b":1}}', '$.a.b') AS value;
-- 输出: "1"
```

**json_tuple**

提取多个字段

```
SELECT json_tuple('{"name": "Alice", "age": 30}', 'name', 'age') AS (name, age);
-- 输出: Alice, 30
```

## 数据治理

技术体系

- 质量

  **统一数仓建模**

  **统一指标管理**

  **统一数据服务**

  **统一产品入口**

- 效率

- 成本

  **计算资源**

  1. 无效任务清理：通过任务生产出来数据的使用情况判断是否为无效任务
  2. 超长任务优化：执行时长
  3. 分散利用计算资源：对就绪时间不敏感的任务（比如算法挖掘、用户标签、数据回刷等）放到非集中运行的时间

  **存储资源**

  1. 数据存储压缩：Hive的txt压缩为ORC格式
  2. 根据表层级调整数据生命周期
  3. 数仓架构优化：统一数仓建模规范，确保每个主题数据只有一份

  **日志采集资源**

  ​	及时下线下游无应用的数据收集任务。

- 安全

  对用户高敏感数据加密

- 元数据

衡量指标

## 数仓建模

**分层考虑因素：**

- 数据粒度：明细，汇总（保留的维度比较多），高维度汇总（保留的维度比较少）
- 通用程度：可复用的数据，个性数据

降低数据计算口径和算法不统一风险。

**分层原则：**

- 从对应用的支持来讲，我们希望越靠上层次，越对应用友好

  这意味着在数据架构中，越是上层的数据，越应该是经过加工和整合的，以便直接用于应用程序和最终用户。数据在这些层级的处理应该使得它们更易于被应用程序查询和使用，从而提高应用性能和用户体验。

- 从能力范围来讲，我们希望即80%的需求由20%的表来支持

  这是指应当优化和精简数据表的设计，以便最常用的数据（占总数据需求的大部分）可以通过少数几个表来提供。这种设计策略有助于提升数据处理的效率和减少维护的复杂性。

- 从数据聚合程度来讲，我们希望，越上层数据的聚合程度越高

  这表示数据层级越高，其包含的信息应越汇总和概括。顶层的数据通常是供决策支持和高级分析使用，聚合数据能更快提供洞见，而底层数据则更详细和原始，适用于深入分析和操作级任务。

**数据域划分规范**

- 划分依据：业务过程。稳定，尽可能覆盖绝大多数的表
- 划分好处：按照数据域垂直拆分，便于开发组织管理
- 关键：词根生产和维护

**公共层模式设计规范(DIM,DWD,DWS)**

- 好处：统一，标准，共享，复用

- 设计理念：

  - 模型灵活性

    通过封装属性，抽出一致性维度，业务的迭代在数据层面只设计维度的调整

  - 数据一致性

    划分严格的数据域和业务过程，在主题建设层面，将业务划分的数据域作为主题，基于业务过程进行维度建模

- 设计原则：

  - 高内聚，低耦合
  - 核心，扩展模型分开
  - 口径一致

  | 层级 | 设计规范                                                     |
  | ---- | ------------------------------------------------------------ |
  | DIM  | 唯一主键；主键统一；维度属性丰富；维表组合、拆分，考虑相关性、时效性、数据量 |
  | DWD  | 业务过程明确；粒度明确，统一；单位统一；严禁反向依赖；时间分区；维度退化；数据域单一；常用度量冗余 |
  | DWS  | 划分主题域；粒度统一；单位统一；严禁反向依赖；避免模型穿透；不建议跨数据域；面向基础维度汇总（app面向高度汇总、个性化）；通用事务性指标、存量类指标（app面向业务自定义的复合型指标和非通用指标） |

- 评估标准

  ![img](https://raw.githubusercontent.com/winway/picRepo/master/pics/modb_20210927_6df0d78e-1f79-11ec-ae4e-00163e068ecd.png)

- 复用度

  ⼀个理想的模型设计，它应该是交织的发散型结构

  ![img](https://raw.githubusercontent.com/winway/picRepo/master/pics/modb_20210927_6e173802-1f79-11ec-ae4e-00163e068ecd.png)

## 数据建模考虑的点是什么，针对一个业务场景，数据模型大致怎么设计？

**了解业务需求** ：与业务团队沟通，了解业务的核心需求和关键性能指标（KPIs）。

**识别数据源** ：确定数据从哪些源头获取，例如销售系统、客户关系管理（CRM）系统等。

**定义事实和维度** ：

**事实表** ：通常包含业务过程中的量化数据。

**维度表** ：提供事实表中度量的上下文信息。

**选择模型类型** ：

**星型模式** ：简单、性能好，适用于大多数业务分析场景。

**雪花模式** ：维度表规范化，适合维度较多的复杂场景。

**建立模型** ：根据业务需求构建事实表和维度表。

**数据整合** ：制定ETL（抽取、转换、加载）流程，整合来自不同源的数据。

**验证与迭代** ：通过与业务用户的反馈迭代优化数据模型。

## 怎么保证数据质量

1. **模型建设**

* **业务过程清晰**：ODS就是原始信息，不修改；DWD面向基础业务过程；DIM描述维度信息；DWS针对最小场景做指标计算；ADS也要分层，面向跨域的建设，和面向应用的建设；
* **高内聚低耦合：** 各主题内数据模型要业务高内聚，避免在一个模型耦合其他业务的指标，造成该模型主题不清晰和性价比低。

```
（1）将业务相近或者相关、粒度相同的数据设计为一个逻辑或者物理模型
（2）将高概率同时访问的数据放在一起，将低概率同时访问的数据分开存储。
```

* **提高模型的复用性和扩展性** ：如果业务过程运行的比较久，过程相对固定，就要尽快下沉到公共层，形成可复用的核心模型；
* **监控模型的稳定性** ：定期检查是否存在数据倾斜或运行时间异常，确保数据处理和查询性能稳定。
* **指标可理解：**按照一定业务事务过程进行业务划分，明细层粒度明确、历史数据可获取，汇总层维度和指标同名同义，能客观反映业务不同角度下的量化程度；

2. **数据成本与性能管理**

* **有效管理数据生命周期** ：通过设定数据的存储周期和访问频率，管理数据的存储成本和性能。
* **优化数据倾斜** ：通过合理的数据分区和负载均衡策略，减少数据倾斜问题，提高查询和处理的效率。
* **控制数据冗余和复制** ：确保数据冗余和复制的策略既能满足性能需求，又不会造成过度的成本负担。

3. **模型数据质量**

* **确保一致性** ：对于存储在不同表中的相同数据，通过定期的数据质量检查，确保没有差异。
* **完整性验证** ：检查数据集是否完整，确保所有必需的数据项都被正确采集和存储。
* **保持数据准确性** ：通过数据校验、清洗和修正流程，确保数据的准确性。
* **维护数据唯一性** ：通过唯一性校验，确保数据表中没有重复的记录，避免错误的数据汇总。
* **提高数据及时性** ：通过优化数据处理流程，确保数据可以及时更新，满足业务需求的实时性或近实时性。
* **规范数据管理** ：统一数据命名和格式规范，确保各个系统和数据表之间的一致性，便于管理和使用。

4. **持续监控和改进**

* **建立数据质量监控（DQC）** ：监控关键数据质量指标，如一致性、完整性、及时性等。
* **定期审查和优化** ：根据数据质量监控结果，不断调整数据处理流程和数据模型，以适应新的业务需求和技术发展。

## 一个复杂的SQL中发生了数据倾斜，你怎么确定是哪个group by还是join发生

1. 查看执行计划：通过查看SQL的执行计划，可以获得SQL的具体执行流程，包括group by和join操作的执行顺序和数据分布情况。如果执行计划显示某个操作的数据分布不均匀，那么很可能是该操作导致了数据倾斜。
2. 分析数据分布情况：可以通过查看相关表的数据分布情况来初步判断是哪个操作导致了数据倾斜。例如，查看group by字段或者join字段的值的分布情况，看是否存在某些值的数量远远超过其他值。
3. 使用统计信息：数据库系统通常会提供统计信息，包括表的行数、列的基数等。可以通过查看统计信息，比较group by字段和join字段的基数，来判断哪个字段的数据分布更倾斜。
4. 使用日志或监控工具：可以通过分析数据库的日志或使用监控工具取量、CPU使用率等，可以初步判断是哪个操作导致了数据倾斜。

# Reference

https://winway.github.io/2022/05/02/bigdata-stack-intro/

一周内反馈，技术面+笔试+2轮技术面+hr面

